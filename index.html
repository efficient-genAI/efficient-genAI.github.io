
<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Tutorial T05: Efficient Text-to-Image and Text-to-3D modeling| [“ECCV 2024 Tutorial”, “September, 29, 2024”, “Milan, Italy”]</title>
<meta name="generator" content="Jekyll v3.9.5" />
<meta property="og:title" content="Efficient Text-to-Image and Text-to-3D modeling" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="[“ECCV 2024 Tutorial”, “September, 29, 2024”, “Milan, Italy”]" />
<meta property="og:description" content="[“ECCV 2024 Tutorial”, “September, 29, 2024”, “Milan, Italy”]" />
<link rel="canonical" href="https://efficient-genAI.github.io/" />
<meta property="og:url" content="https://efficient-genAI.github.io/" />
<meta property="og:site_name" content="Efficient Text-to-Image and Text-to-3D modeling" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Efficient Text-to-Image and Text-to-3D modeling" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"[“ECCV 2024 Tutorial”, “September, 29, 2024”, “Milan, Italy”]","headline":"Efficient Text-to-Image and Text-to-3D modeling","name":"Efficient Text-to-Image and Text-to-3D modeling","url":"https://efficient-genAI.github.io/"}</script>
<!-- End Jekyll SEO tag -->

    <meta property="og:title" content='SynData4CV-CVPR2024'/>
    <meta property="og:image" content="efficient-genAI.github.io/milan.jpg">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="200">
    <meta property="og:image:height" content="200">
    <meta property="og:type" content='website'/>
    <meta name="description" content="ECCV 2024 TutorialSeptember, 2024Milan, Italy"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#39275B">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=6e64313460609e93346c4862a60afa3aebec1d2f">
    
  </head>
  <body>
  <style> 
    .logo {
      display: block;
      margin-left: auto;
      margin-right: auto;
      width: 50%;
      background-color: #000000; 
    } 
    
    .page-header1 {
  background-image: url('milan.jpg');
  background-repeat: no-repeat;
  background-size: 100% ;
  background-position: center;
  padding-bottom: 1rem;
  background-color: #ffffff;
    }
  .page-header2 {
  background-color: #ffffff;
  background-image: linear-gradient(120deg, #ffffff, #ffffff);
  }
    .btn {
  display: inline-block;
  margin-bottom: 1rem;
  color: rgba(0, 0, 0, 0.897);
  background-color: rgba(0, 0, 0, 0.8);
  border-color: rgba(0, 0, 0, 0.918);
  border-style: solid;
  border-width: 1px;
  border-radius: 0.3rem;
  transition: color 0.2s, background-color 0.2s, border-color 0.2s;
    }
  </style>
    <section class="page-header page-header1">
      <!-- <img alt="logo" src="pics/placehold-logo.svg" class = "logo" > -->
      <h1 class="project-name" style="color:#ffffff; ">Synthetic Data for Computer Vision</h1>
     <h2 class="project-tagline" style="font-size: 32px; color:#ffffff ;  opacity: 100%; "><span>CVPR 2024 Workshop</span><br><span>June, 2024</span><br><span>Seattle, United States</span><br></h2>
   
      <br>
      <br>
      <br>
      <br>
      <br>
      <br>
       
       
    <a href="./index.html" class="btn" style="color:#ffffff ; font-size: large; text-shadow: -0.1px -0.1px 0 #000000, 0.1px -0.1px 0 #000000, -0.1px 0.1px 0 #000000, 0.1px 0.1px 0 #000000;">Overview</a>
  
       
    <a href="./index.html#speakers" class="btn" style="color:#ffffff ; font-size: large; text-shadow: -0.1px -0.1px 0 #000000, 0.1px -0.1px 0 #000000, -0.1px 0.1px 0 #000000, 0.1px 0.1px 0 #000000;">Invited Speakers</a>
  
       
    <a href="./index.html#schedule" class="btn" style="color:#ffffff ; font-size: large; text-shadow: -0.1px -0.1px 0 #000000, 0.1px -0.1px 0 #000000, -0.1px 0.1px 0 #000000, 0.1px 0.1px 0 #000000;">Schedule</a>

    </section>
    
   
    <section class="main-content">
      <style> 
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 75%;
} </style>

<h1 id="overview">Overview</h1>
<div style="text-align: left; max-width: 800px; margin: auto;">
The workshop aims to explore the use of synthetic data in training and evaluating computer vision models, as well as in other related domains. During the last decade, advancements in computer vision were catalyzed by the release of painstakingly curated human-labeled datasets. Recently, people have increasingly resorted to synthetic data as an alternative to laborintensive human-labeled datasets for its scalability, customizability, and costeffectiveness. Synthetic data offers the potential to generate large volumes of diverse and high-quality vision data, tailored to specific scenarios and edge cases that are hard to capture in real-world data. However, challenges such as the domain gap between synthetic and real-world data, potential biases in synthetic generation, and ensuring the generalizability of models trained on synthetic data remain. We hope the workshop can provide a forum to discuss and encourage further exploration in these areas.
</div>

<h1 id="peakers">Invited Speakers</h1>
<ul>
  <li>The speakers haven’t been finalized, stay tuned for updates!</li>
</ul>
<div style="display: flex; flex-wrap: wrap; justify-content: space-around;">

  <div style="width:45%; margin: 1%;">
    <a href="https://anikem.github.io/">
      <img alt="Ani	Kembhavi" src="pics/speakers/ani.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
  <a href="https://anikem.github.io/">Ani	Kembhavi</a><br />
    Allen Institute for AI (AI2)
  </div>
  
  <div style="width:45%; margin: 1%;">
    <a href="https://www.cs.princeton.edu/~jiadeng/">
      <img alt="Jia Deng" src="pics/speakers/jiadeng.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
  <a href="https://www.cs.princeton.edu/~jiadeng/"> Jia Deng</a><br />
    Princeton University
  </div>
  
  <div style="width:45%; margin: 1%;">
    <a href="https://people.csail.mit.edu/ludwigs/">
      <img alt="Ludwig Schmidt" src="pics/speakers/ludwig.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
  <a href="https://people.csail.mit.edu/ludwigs/">Ludwig Schmidt</a><br />
    University of Washington
  </div>
  
   <div style="width:45%; margin: 1%;">
    <a href="https://www.cs.umd.edu/people/lin">
      <img alt="Ming Lin" src="pics/speakers/minglin.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
  <a href="https://www.cs.umd.edu/people/lin">Ming Lin</a><br />
    University of Maryland
  </div>
  
  <!-- <div style="width:45%; margin: 1%;">
    <a href="https://www.cs.princeton.edu/~olgarus/">
      <img alt="Olga Russakovsky" src="pics/speakers/olgarussakovsky.jpg"  height="200"   width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://www.cs.princeton.edu/~olgarus/">Olga Russakovsky</a><br>
    Princeton University
  </div>
  
  <div style="width:45%; margin: 1%;">
    <a href="https://research.google/people/oriol-vinyals/">
      <img alt="Oriol Vinyals" src="pics/speakers/oriol.jpg" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://research.google/people/oriol-vinyals/">Oriol Vinyals</a><br>
    Google DeepMind
  </div> -->

  <div style="width:45%; margin: 1%;">
    <a href="https://www.cs.cmu.edu/~rsalakhu/">
      <img alt="Ruslan Salakhutdinov" src="pics/speakers/ruslan.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
    <a href="https://www.cs.cmu.edu/~rsalakhu/">Ruslan Salakhutdinov</a><br />
    Carnegie Mellon University
  </div>



  <div style="width:45%; margin: 1%;">
    <a href="https://people.csail.mit.edu/yalesong/home/">
      <img alt="Yale Song" src="pics/speakers/yale.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
    <a href="https://people.csail.mit.edu/yalesong/home/">Yale Song</a><br />
    FAIR, Meta AI
  </div>

  <div style="width:45%; margin: 1%;">
    <a href="https://www.skamalas.com/">
      <img alt="Yannis Kalantidis" src="pics/speakers/yannis.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover; " />
    </a><br />
    <a href="https://www.skamalas.com/">Yannis Kalantidis</a><br />
    NAVER LABS Europe
  </div>
   



  <div style="width:45%; margin: 1%;">
  <a href="">
   
  </a><br />
  <a href=""></a><br />
  
</div>
</div>

<h1 id="schedule">Schedule</h1>

<ul>
  <li><strong>Date:</strong> June 18, 2024</li>
  <li><strong>Time:</strong> 8:30 AM - 5:30 PM</li>
  <li><strong>Location:</strong> Summit 423-425</li>
</ul>

<table>
  <tr><td>Times (PST)</td><td>Event</td></tr>
  <tr><td>09:00 - 09:10</td><td>Opening</td></tr>
  <tr><td>09:10 - 09:50</td><td>Talk by Ludwig Schmidt</td></tr>
  <tr><td>09:50 - 10:30</td><td>Talk by Ruslan Salakhutdinov</td></tr>
  <tr><td>10:30 - 10:50</td><td>Break</td></tr>
  <tr><td>10:50 - 11:30</td><td>Talk by Yale Song</td></tr>
  <tr><td>11:30 - 12:10</td><td>Talk by Jia Deng</td></tr>
  <tr><td>12:10 - 13:30</td><td>Lunch</td></tr>
  <tr><td>13:30 - 14:30</td><td>Poster Session</td></tr>
  <tr><td>14:30 - 15:10</td><td>Talk by Ani	Kembhavi</td></tr>
  <tr><td>15:10 - 15:50</td><td>Talk by Ming Lin</td></tr>
  <tr><td>15:50 - 16:10</td><td>Break</td></tr>
  <tr><td>16:10 - 16:50</td><td>Talk by Yannis Kalantidis</td></tr>
  <tr><td>16:50 - 17:05</td><td>Oral Presentation: CinePile: A Long Video Question Answering Dataset and Benchmark</td></tr>
  <tr><td>17:05 - 17:20</td><td>Oral Presentation: GenAI-Bench: A Holistic Benchmark for Compositional Text-to-Visual Generation</td></tr>
  <tr><td>17:20 - 17:30</td><td>Closing</td></tr>
</table>

<h1 id="poster-session">Poster Session</h1>
<p>Notice: The location of the poster session is different from the workshop.</p>
<ul>
  <li><strong>Date:</strong> June 18, 2024</li>
  <li><strong>Time:</strong> 1:30 PM - 2:30 PM</li>
  <li><strong>Location:</strong> Arch Building Exhibit Hall, #300 - 349</li>
</ul>

<h1 id="awards">Awards</h1>

<p><strong>Best long paper</strong></p>
<table> 
  <tr><td><a href="https://openreview.net/forum?id=3Bv2Sz54lV">CinePile: A Long Video Question Answering Dataset and Benchmark</a>. Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, Tom Goldstein</td></tr>
</table>

<p><strong>Long paper honorable mention</strong></p>
<table> 
  <tr><td><a href="https://openreview.net/forum?id=p6igw3ldIc">A Benchmark Synthetic Dataset for C-SLAM in Service Environments</a>. Harin Park, Inha Lee, Minje Kim, Hyungyu Park, Kyungdon Joo</td></tr>
</table>

<p><strong>Best short paper</strong></p>
<table> 
  <tr><td><a href="https://openreview.net/forum?id=hJm7qnW3ym">GenAI-Bench: A Holistic Benchmark for Compositional Text-to-Visual Generation</a>. Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Emily Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan</td></tr>
</table>

<p><strong>Short paper honorable mention</strong></p>
<table> 
  <tr><td><a href="https://openreview.net/forum?id=B7PKwbqNdo">R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding</a>. Qirui Wu, Sonia Raychaudhuri, Daniel Ritchie, Manolis Savva, Angel X Chang</td></tr>
</table>

<h1 id="accepted-papers">Accepted Papers</h1>
<table>
<tr><td><a href="https://openreview.net/forum?id=FZxofmVOwg">DDOS: The Drone Depth and Obstacle Segmentation Dataset</a>. Benedikt Kolbeinsson, Krystian Mikolajczyk</td></tr>
<tr><td><a href="https://openreview.net/forum?id=cesQERAXhi">From NeRF to 3DGS: A Leap in Stereo Dataset Quality?</a>. Magnus Kaufmann Gjerde, Filip Slezák, Joakim Bruslund Haurum, Thomas B. Moeslund</td></tr>
<tr><td><a href="https://openreview.net/forum?id=T1eCXQuWE2">Training Robust Classifiers with Diffusion Denoised Examples</a>. Chandramouli Shama Sastry, Sri Harsha Dumpala, Sageev Oore</td></tr>
<tr><td><a href="https://openreview.net/forum?id=wNKOQVcMDS">Uncertainty Inclusive Contrastive Learning for Leveraging Synthetic Images</a>. Fiona Cai, Emily Mu, John Guttag</td></tr>
<tr><td><a href="https://openreview.net/forum?id=WlBqPK2uLa">HDL-SAM: A Hybrid Deep Learning Framework for High-Resolution Imaging in Scanning Acoustic Microscopy</a>. Akshit Sharma, Ayush Somani, Pragyan Banerjee, Frank Melandsø, Anowarul Habib</td></tr>
<tr><td><a href="https://openreview.net/forum?id=25vcM0vPyD">MICDrop: Masking Image and Depth Features via Complementary Dropout for Domain-Adaptive Semantic Segmentation</a>. Linyan Yang, Lukas Hoyer, Mark Weber, Tobias Fischer, Dengxin Dai, Laura Leal-Taixé, Daniel Cremers, Marc Pollefeys, Luc Van Gool</td></tr>
<tr><td><a href="https://openreview.net/forum?id=XQQ12dIR7C">An Approach to Synthesize Thermal Infrared Ship Images</a>. Doan Thinh Vo, Phan Anh Đức, Nguyen Nhu Thao, Huong Ninh</td></tr>
<tr><td><a href="https://openreview.net/forum?id=Skh18goLXF">LAESI: Leaf Area Estimation with Synthetic Imagery</a>. Jacek Kałużny, Yannik Schreckenberg, Karol Cyganik, Peter Annighöfer, Soren Pirk, Dominik Michels, Mikolaj Cieslak, Farhah Assaad, Bedrich Benes, Wojtek Palubicki</td></tr>
<tr><td><a href="https://openreview.net/forum?id=hJm7qnW3ym">GenAI-Bench: A Holistic Benchmark for Compositional Text-to-Visual Generation</a>. Baiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Emily Li, Xide Xia, Graham Neubig, Pengchuan Zhang, Deva Ramanan</td></tr>
<tr><td><a href="https://openreview.net/forum?id=I1vrc56Vau">SEVD: Synthetic Event-based Vision Dataset for Ego and Fixed Traffic Perception</a>. Manideep Reddy Aliminati, Bharatesh Chakravarthi, Aayush Atul Verma, Arpitsinh Vaghela, Hua Wei, Xuesong Zhou, Yezhou Yang</td></tr>
<tr><td><a href="https://openreview.net/forum?id=7YnS0mYl8s">Training with Real instead of Synthetic Generated Images Still Performs Better</a>. Scott Geng, Ranjay Krishna, Pang Wei Koh</td></tr>
<tr><td><a href="https://openreview.net/forum?id=SwO84a6yA5">A Neural Model for High-Performance Scanning Electron Microscopy Image Simulation of Porous Materials</a>. Tim Dahmen, Markus Kronenberger, Niklas Rottmayer, Katja Schladitz, Claudia Redenbach</td></tr>
<tr><td><a href="https://openreview.net/forum?id=i5tZ9szbTt">S2MGen: A Synthetic Skin Mask Generator for Improving Segmentation</a>. Subhadra Gopalakrishnan, Trisha Mittal, Jaclyn Pytlarz, Yuheng Zhao</td></tr>
<tr><td><a href="https://openreview.net/forum?id=48zldWWWNi">Self-Distillation on Conditional Spatial Activation Maps for ForeGround-BackGround Segmentation</a>. Yeruru Asrar Ahmed, Anurag Mittal</td></tr>
<tr><td><a href="https://openreview.net/forum?id=A9NOAS0hn1">GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning</a>. Mehran Kazemi, Hamidreza Alvari, Ankit Anand, Jialin Wu, Xi Chen, Radu Soricut</td></tr>
<tr><td><a href="https://openreview.net/forum?id=gYE6LBojgM">CompoDiff: Versatile Composed Image Retrieval With Latent Diffusion</a>. Geonmo Gu, Sanghyuk Chun, Wonjae Kim, HeeJae Jun, Yoohoon Kang, Sangdoo Yun</td></tr>
<tr><td><a href="https://openreview.net/forum?id=4xURVbKlCt">Video2Game: Real-time, Interactive, Realistic and Browser-Compatible Environment from a Single Video</a>. Hongchi Xia, Zhi-Hao Lin, Wei-Chiu Ma, Shenlong Wang</td></tr>
<tr><td><a href="https://openreview.net/forum?id=0sMJfHCyk2">UrbanIR: Large-Scale Urban Scene Inverse Rendering from a Single Video</a>. Zhi-Hao Lin, Bohan Liu, Yi-Ting Chen, David Forsyth, Jia-Bin Huang, Anand Bhattad, Shenlong Wang</td></tr>
<tr><td><a href="https://openreview.net/forum?id=NcCAy6PXNF">DISC: Latent Diffusion Models with Self-Distillation from Separated Conditions for Prostate Cancer Grading</a>. Man M. Ho, Elham Ghelichkhan, Yosep Chong, Yufei Zhou, Beatrice S. Knudsen, Tolga Tasdizen</td></tr>
<tr><td><a href="https://openreview.net/forum?id=8cd6gOPm7o">On the Equivalency, Substitutability, and Flexibility of Synthetic Data</a>. Che-Jui Chang, Danrui Li, Seonghyeon Moon, Mubbasir Kapadia</td></tr>
<tr><td><a href="https://openreview.net/forum?id=BgpApqspGw">Beyond Internet Images: Evaluating Vision-Language Models for Domain Generalization on Synthetic-to-Real Industrial Datasets</a>. Louis Hémadou, Héléna Vorobieva, Ewa Kijak, Frederic Jurie</td></tr>
<tr><td><a href="https://openreview.net/forum?id=jSB5wlUU3p">DiffInject: Revisiting Debias via Synthetic Data Generation using Diffusion-based Style Injection</a>. Donggeun Ko, Sangwoo Jo, Dongjun Lee, Namjun Park, Jaekwang KIM</td></tr>
<tr><td><a href="https://openreview.net/forum?id=ckhhCcGq9n">Balancing Quality and Quantity: The Impact of Synthetic Data on Smoke Detection Accuracy in Computer Vision</a>. Ethan Seefried, Changsoo Jung, Jack Fitzgerald, Mariah Bradford, Trevor Chartier, Nathaniel Blanchard</td></tr>
<tr><td><a href="https://openreview.net/forum?id=XFYqhBBB61">Object-Conditioned Energy-Based Model for Attention Map Alignment in Text-to-Image Diffusion Models</a>. Yasi Zhang, Peiyu Yu, Ying Nian Wu</td></tr>
<tr><td><a href="https://openreview.net/forum?id=WgRrHFlyLU">DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control</a>. Yuru Jia, Lukas Hoyer, Shengyu Huang, Tianfu Wang, Luc Van Gool, Konrad Schindler, Anton Obukhov</td></tr>
<tr><td><a href="https://openreview.net/forum?id=3Bv2Sz54lV">CinePile: A Long Video Question Answering Dataset and Benchmark</a>. Ruchit Rawal, Khalid Saifullah, Ronen Basri, David Jacobs, Gowthami Somepalli, Tom Goldstein</td></tr>
<tr><td><a href="https://openreview.net/forum?id=ZV9XDgmTvf">m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks</a>. Zixian Ma, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</td></tr>
<tr><td><a href="https://openreview.net/forum?id=EZYvU2oC6J">Harlequin: Color-driven Generation of Synthetic Data for Referring Expression Comprehension</a>. Luca Parolari, Elena Izzo, Lamberto Ballan</td></tr>
<tr><td><a href="https://openreview.net/forum?id=J0p8qKsWl0">Inclusive Portrait Lighting Estimation Model Leveraging Graphic-Based Synthetic Data</a>. Kin Ching Lydia Chau, Tao LI, Ruowei Jiang, Zhi Yu, Panagiotis-Alexandros Bokaris</td></tr>
<tr><td><a href="https://openreview.net/forum?id=k4Xnh0EPus">Attributed Synthetic Data Generation for Zero-shot Image Classification</a>. Shijian Wang, Linxin Song, Ryotaro Shimizu, Masayuki Goto, Hanqian wu</td></tr>
<tr><td><a href="https://openreview.net/forum?id=p6igw3ldIc">A Benchmark Synthetic Dataset for C-SLAM in Service Environments</a>. Harin Park, Inha Lee, Minje Kim, Hyungyu Park, Kyungdon Joo</td></tr>
<tr><td><a href="https://openreview.net/forum?id=XusBKwfVxX">Compositional Learning of Visually-Grounded Concepts Using Reinforcement</a>. Zijun Lin, Haidi Azaman, M Ganesh Kumar, Cheston Tan</td></tr>
<tr><td><a href="https://openreview.net/forum?id=Uq6uBK6n5d">Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models</a>. Yushi Hu, Otilia Stretcu, Chun-Ta Lu, Krishnamurthy Viswanathan, Kenji Hata, Enming Luo, Ranjay Krishna, Ariel Fuxman</td></tr>
<tr><td><a href="https://openreview.net/forum?id=cAFgDFprbC">DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback</a>. Jiao Sun, Deqing Fu, Yushi Hu, Su Wang, Royi Rassin, Da-Cheng Juan, Dana Alon, Charles Herrmann, Sjoerd van Steenkiste, Ranjay Krishna, Cyrus Rashtchian</td></tr>
<tr><td><a href="https://openreview.net/forum?id=EvDZlcHNDo">SIFTer: Self-improving Synthetic Datasets for Pre-training Classification Models</a>. Ryo Hayamizu, Shota Nakamura, Sora Takashima, Hirokatsu Kataoka, Ikuro Sato, Nakamasa Inoue, Rio Yokota</td></tr>
<tr><td><a href="https://openreview.net/forum?id=B7PKwbqNdo">R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding</a>. Qirui Wu, Sonia Raychaudhuri, Daniel Ritchie, Manolis Savva, Angel X Chang</td></tr>
<tr><td><a href="https://openreview.net/forum?id=xHKWN3Yi6U">Intrinsic LoRA: A Generalist Approach for Discovering Knowledge in Generative Models</a>. Xiaodan Du, Nicholas Kolkin, Greg Shakhnarovich, Anand Bhattad</td></tr>
<tr><td><a href="https://openreview.net/forum?id=4tbbxtGwou">XIMAGENET-12: An Explainable Visual Benchmark Dataset for Model Robustness Evaluation</a>. Qiang Li, Dan Zhang, Shengzhao Lei, Xun Zhao, WeiWei Li, Porawit Kamnoedboon, Junhao Dong, Shuyan Li</td></tr>
<tr><td><a href="https://openreview.net/forum?id=D9sfy3NUbY">Paved2Paradise: Cost-Effective and Scalable LiDAR Simulation by Factoring the Real World</a>. Michael A. Alcorn, Noah Schwartz</td></tr>
<tr><td><a href="https://openreview.net/forum?id=X2cmDAjprN">Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation: Do We Need Artificial Augmentation?</a>. Dmitry Yu. Ignatov, Andrey Ignatov, Radu Timofte</td></tr>
<tr><td><a href="https://openreview.net/forum?id=oKwYycMSrf">SynthCLIP: Are We Ready for a Fully Synthetic CLIP Training?</a>. Hasan Abed Al Kader Hammoud, Hani Itani, Fabio Pizzati, Adel Bibi, Bernard Ghanem</td></tr>
<tr><td><a href="https://openreview.net/forum?id=VSFR5eBP7h">Implicit Neural Clustering</a>. Thomas Kreutz, Max Mühlhäuser, Alejandro Sanchez Guinea</td></tr>
</table>

<!-- <div style="display: flex">
  <div style="width:22.5%">
    <a href="https://eshed1.github.io/">
    <img alt="Eshed Ohn-Bar" src="pics/eshed_ohn_bar.jpg" height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
    <a href="https://eshed1.github.io/">Eshed Ohn-Bar</a><br>
    Boston University
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="https://home.cs.colorado.edu/~DrG/AboutMe.html">
    <img alt="Danna Gurari" src="pics/danna_gurari.jpg"  height="200"   width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://home.cs.colorado.edu/~DrG/AboutMe.html">Danna Gurari</a><br>
    University of Colorado Boulder
  </div>
  
    <div style="width:2.5%">
  </div>
       
  <div style="width:22.5%">
    <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-chiekoa">
    <img alt="Chieko Asakawa" src="pics/chieko_asakawa.jpg"   height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://researcher.watson.ibm.com/researcher/view.php?person=us-chiekoa">Chieko Asakawa</a><br>
    Carnegie Mellon University and IBM
  </div>
  
    <div style="width:2.5%">
  </div>

  <div style="width:22.5%">
    <a href="https://ischool.umd.edu/directory/hernisa-kacorri/">
    <img alt="Hernisa Kacorri" src="pics/Hernisa-Kacorri.jpg"   height="200" width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://ischool.umd.edu/directory/hernisa-kacorri/">Hernisa Kacorri</a><br>
    University of Maryland
  </div>
  
    <div style="width:2.5%">
  </div>
  
    <div style="width:22.5%">
    <a href="http://www.cs.cmu.edu/~kkitani/">
    <img alt="Kris Kitani" src="pics/kitani_kris.jpg"  height="200"  width ="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a><br>
    Carnegie Mellon University
  </div>

</div> -->

<!-- ## Advising committee -->

<!-- <div style="display: flex">
 <div style="width:22.5%">
    <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">
    <img alt="name_16" src="pics/placeholder.jpg"  height="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">[Name]</a><br>
    [Institution]
  </div>
  
  <div style="width:2.5%">
  </div>
   
  <div style="width:22.5%">
    <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">
    <img alt="name_16" src="pics/placeholder.jpg"  height="200" style =  "border-radius: 50%; object-fit: cover; ">
    </a><br>
  <a href="https://staging-temp-site.github.io/staging-temp-site.gitub.io/">[Name]</a><br>
    [Institution]
  </div>
</div> -->

<!-- ## Program Committee -->
<!-- 
| --- | --- |
|  |  | -->

<!-- ## Student Organizers -->
<!-- 
| --- | --- |
|  |  |
 -->

<!-- ## Call for papers -->
<!-- Please refer to the **[call for papers](./call-for-papers.html)** page for more details. -->

<!-- 
<div style="text-align: center">
<u><g8>Challenge</g8></u>
</div>
 -->

<!-- ## Challenge overview -->
<!-- 
<div style="text-align: justify">


Towards building a community of accessibility research in computer vision conferences, we introduce a computer vision challenge with synthetic and real-world benchmarks. The challenge (based on our ICCV’21 paper, <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>) will be used to benchmark various computer vision tasks when comparing new and established methods for fine-grained perception of tasks relevant to people with disabilities. The challenge is designed in the spirit of various other vision challenges that help advance the state-of-the-art of computer vision for autonomous systems, e.g., in robust vision (CVPR’21), human action recognition trajectory forecasting (CVPR’21), etc. E
 </div>
<div class = "center">
    <img alt="fig1" src="pics/fig1.svg" >
    <p>Fig. 1: An interactive simulation environment will be used as part of the workshop challenge for training machine perception and learning models in the context of accessibility (taken from <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>).</p>
<br> 
<div class = "center">
    <img alt="fig2" src="pics/fig2.svg" >
    <p>An example from the instance segmentation challenge for perceiving people with mobility aids.</p>
</div>
<br> 
</div>
<br>-->

<!-- ## Challenge Organization

<div style="display: flex">
  <div style="width:22.5%">
    <a href="mailto:sgzk@bu.edu">
    <img alt="Zhongkai Shangguan" src="pics/zhongkai_shangguan.png"   style =  "border-radius: 50%; object-fit: cover; width = 100% ">
    </a><br>
  <a href="mailto:sgzk@bu.edu">Zhongkai Shangguan</a><br>
    Boston University
  </div>
  
    <div style="width:2.5%">
  </div>
  
  <div style="width:22.5%">
    <a href="mailto:zhangjim@bu.edu">
    <img alt="Jimuyang Zhang" src="pics/jimuyang_zhang.jpg"  style =  "border-radius: 50%; object-fit: cover; width = 100% ">
    </a><br>
    <a href="mailto:zhangjim@bu.edu">Jimuyang Zhang</a><br>
    Boston University
  </div>

</div> -->

<!-- ## Challenge

<div style="text-align: justify">
  
  <strong>As an updated challenge for 2023, we release the following:</strong>
  
  <ol>
  <li>Training, validation, and testing data, which can be found in <a href="https://drive.google.com/drive/folders/12e-Qom2qQWF7brBu36sIQZWfj8kTBtj-?usp=share_link">this link</a></li>
    <li>An evaluation server <a href="https://eval.ai/web/challenges/challenge-page/1998/overview">for instance segmentation</a> and <a href="https://eval.ai/web/challenges/challenge-page/2001/overview">for pose estimation.</a></li>
  </ol>
  
  More info on data and submission can be found in the eval.ai links above. Note that the data this year includes both instance segmentation and pose estimation challenge. Moreover, we provide access to temporal history and LiDAR data for each image.
  
  <br>
  The challenge builds on our prior workshop's synthetic instance segmentation benchmark with mobility aids (see Zhang et al., X-World: Accessibility, Vision, and Autonomy Meet, ICCV 2021 <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_X-World_Accessibility_Vision_and_Autonomy_Meet_ICCV_2021_paper.pdf">bit.ly/2X8sYoX</a>). The benchmark contains challenging accessibility-related person and object categories, such as `cane' and `wheelchair.' We aim to use the challenge to uncover research opportunities and spark the interest of computer vision and AI researchers working on more robust visual reasoning models for accessibility. 
  
<div class = "center">
    <img alt="fig2" src="pics/i1.jpg" >
    <p>An example from the instance segmentation challenge for perceiving people with mobility aids.</p>
</div>
  <div class = "center">
    <img alt="fig2" src="pics/pose_xworld.png" >
    <p>An example from the pose challenge added in 2023.</p>
</div>
  <br>
  The team with the top performing submission will be invited to give short talks during the workshop and will receive a financial award of <b>$500</b> and an <a href="https://store.opencv.ai/products/oak-d">OAK—D camera</a> (We thank the National Science Foundation, US Department of Transportation's Inclusive Design Challenge and Intel for their support for these awards) 
  <br><br>
</div> -->

<h1 id="call-for-papers">Call for Papers</h1>

<!-- <div style="text-align: justify">
We encourage submission of relevant research (including work in progress, novel perspectives, formative studies, benchmarks, methods) as extended abstracts for the poster session and workshop discussion (up to 4 pages in CVPR format, not including references). CVPR Overleaf template can be <a href="https://www.overleaf.com/latex/templates/cvpr-2022-author-kit/qbmjsdxryffn">found here</a>. Latex/Word templates can be <a href="https://cvpr2022.thecvf.com/sites/default/files/2021-10/cvpr2022-author_kit-v1_1-1.zip">found here</a>. Please send your extended abstracts to <a href="mailto:mobility@bu.edu">mobility@bu.edu</a>. Note that submissions do not need to be anonymized. Extended abstracts of already published works can also be submitted. Accepted abstracts will be presented at the poster session, and will not be included in the printed proceedings of the workshop.
Topics of interests by this workshop include, but are not limited to:
  <br>
  <ol>
  <li>AI for Accessibility</li>
  <li>Accessibility-Centered Computer Vision Tasks and Datasets</li>
  <li>Data-Driven Accessibility Tools, Metrics and Evaluation Frameworks</li>
  <li>Practical Challenges in Ability-Based Assistive Technologies</li>  
  <li>Accessibility in Robotics and Autonomous Vehicles</li>  
  <li>Long-Tail and Low-Shot Recognition of Accessibility-Based Tasks</li>  
  <li>Accessible Homes, Hospitals, Cities, Infrastructure, Transportation</li>   
  <li>Crowdsourcing and Annotation Tools for Vision and Accessibility</li>  
  <li>Empirical Real-World Studies in Inclusive System Design</li>  
  <li>Assistive Human-Robot Interaction</li>  
  <li>Remote Accessibility Systems</li>   
  <li>Multi-Modal (Audio, Visual, Inertial, Haptic) Learning and Interaction</li>  
  <li>Accessible Mobile and Information Technologies</li>  
  <li>Virtual, Augmented, and Mixed Reality for Accessibility</li>  
  <li>Novel Designs for Robotic, Wearable and Smartphone-Based Assistance</li>  
  <li>Intelligent Assistive Embodied and Navigational Agents</li>   
  <li>Socially Assistive Mobile Applications</li>  
  <li>Human-in-the-Loop Machine Learning Techniques</li>  
  <li>Accessible Tutoring and Education</li>  
  <li>Personalization for Diverse Physical, Motor, and Cognitive Abilities</li>  
  <li>Embedded Hardware-Optimized Assistive Systems</li>  
  <li>Intelligent Robotic Wheelchairs</li>  
  <li>Medical and Social and Cultural Models of Disability</li>  
  <li>New Frameworks for Taxonomies and Terminology</li>  
    </ol>
</div> -->
<p>We invite papers on <strong>the use of synthetic data for training and
evaluating computer vision models.</strong> We welcome submissions along two
tracks:</p>

<ul>
  <li>
    <p><strong>Full papers:</strong> Up to 8 pages, not including references/appendix.</p>
  </li>
  <li>
    <p><strong>Short papers:</strong> Up to 4 pages, not including references/appendix.</p>
  </li>
</ul>

<p>Accepted papers will be allocated a poster presentation and displayed on
the workshop website. In addition, we will offer a Best Long Paper
award, Best Paper Runner-up award, and Best Short Paper with oral
presentation.</p>

<h3 id="topics">Topics</h3>

<p>Potential topics include, but are not limited to:</p>

<ul>
  <li>
    <p><strong>Effectiveness:</strong> What is the most effective way to generate and
  leverage synthetic data? How "realistic" does synthetic data need
  to be?</p>
  </li>
  <li>
    <p><strong>Efficiency and scalability:</strong> Can we make synthetic data
  generation more efficient and scalable without sacrificing quality?</p>
  </li>
  <li>
    <p><strong>Benchmark and evaluation:</strong> What benchmark and evaluation methods
  are needed to assess the efficacy of synthetic data for computer
  vision?</p>
  </li>
  <li>
    <p><strong>Risks and ethical considerations:</strong> What ethical questions and
  risks are associated with synthetic data (<em>e.g.</em> bias
  amplification), and how can we address them?</p>
  </li>
  <li>
    <p><strong>Applications:</strong> In addition to existing attempts on leveraging
  synthetic data for training visual recognition and vision-language
  models, what are other tasks in computer vision or other related
  fields (<em>e.g.</em>, robotics, NLP) that could benefit from synthetic
  data?</p>
  </li>
  <li>
    <p><strong>Other open problems:</strong> How do we decide which type of data to use,
  synthetic or real-world data? What is the optimal way to combine
  both if both are available? How much real-world data do we need (in
  the long run)?</p>
  </li>
</ul>

<h3 id="submission-instructions">Submission Instructions</h3>

<p>Submissions should be anonymized and formatted using the <a href="http://google.com">CVPR 2024
template</a> and uploaded as a single PDF. 
Note that our workshop is non-archival.<br />
<br />
<strong>Submission link:</strong> <a href="https://openreview.net/group?id=thecvf.com/CVPR/2024/Workshop/SynData4CV">OpenReview Link</a></p>

<h1 id="important-workshop-dates">Important workshop dates</h1>
<!-- - Updated challenge release: <strong>3/18/2023</strong>
- Workshop abstract submission deadline: <strong>6/11/2023</strong> (11:59PM PST, please submit extended abstracts via email to mobility@bu.edu) 
- Challenge submission deadline: <strong>6/11/2023</strong> 
- Abstract notification: <strong>6/13/2023</strong>
- Challenge winner announcement: <strong>6/18/2023</strong>
- TBD -->
<ul>
  <li>Deadline for submission: <del><strong>March 15th, 11:59 PM Pacific Time</strong></del> <strong>March 30th, 11:59 PM Pacific Time</strong></li>
  <li>Notification of acceptance: <strong>April 9th, 11:59 PM Pacific Time</strong></li>
  <li>Camera Ready submission deadline: <strong>April 24th, 11:59 PM Pacific Time</strong></li>
  <li>Workshop date: <strong>June 18th, 2024 (Full day)</strong></li>
</ul>

<!-- ### Join our **[mailing list](https://staging-temp-site.github.io/staging-temp-site.gitub.io/)** for updates. -->

<!-- ## Videos -->

<!-- <div style=" float: center;">
    <div align="center" style="width:45%; float: left;">
      <h4><u>OpenGuide</u> </h4>
        <iframe src="https://www.youtube.com/embed/mGq9sL1spzc" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px);height: 30vh" allowfullscreen></iframe>
    </div>
    <div style="width:5%; float: left;">
        <p></p>
    </div>
    
    <!--div align="center"  style="width:45%; float: left;">
      <h4 ><u>X-World</u> </h4>
      
        <iframe src="https://www.youtube.com/embed/z_YwWIZWg58" frameborder="0"
          allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
          style="width:100%; clip-path:inset(1px 1px); height: 30vh" allowfullscreen></iframe>
      
    </div>
  </div-->

<h1 id="related-workshops">Related Workshops</h1>
<ul>
  <li><a href="https://syntml-cvpr2022-workshop.github.io/">Machine Learning with Synthetic Data @ CVPR 2022</a></li>
  <li><a href="https://sites.google.com/view/sdas2023/">Synthetic Data for Autonomous Systems @ CVPR 2023</a></li>
  <li><a href="https://www.syntheticdata4ml.vanderschaar-lab.com/">Synthetic Data Generation with Generative AI @ NeurIPS 2023</a></li>
</ul>

<h1 id="organizers">Organizers</h1>
<div style="display: flex; flex-wrap: wrap; justify-content: space-around;">
  
  <!-- Organizer 1 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://jieyuz2.github.io/">
      <img alt="Jieyu Zhang" src="pics/organizers/jieyuzhang.png" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://jieyuz2.github.io/">Jieyu Zhang</a><br />
    University of Washington
  </div>

  <!-- Organizer 2 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://chengyuhsieh.github.io/">
      <img alt="Cheng-Yu Hsieh" src="pics/organizers/chengyuhsieh.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://chengyuhsieh.github.io/">Cheng-Yu Hsieh</a><br />
    University of Washington
  </div>

  <!-- Organizer 3 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://zixianma.github.io/">
      <img alt="Zixian Ma" src="pics/organizers/zixianma.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://zixianma.github.io/">Zixian Ma</a><br />
    University of Washington
  </div>

  <!-- Organizer 4 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://ssundaram21.github.io/">
      <img alt="Shobhita Sundaram" src="pics/organizers/ssundaram.png" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://ssundaram21.github.io/">Shobhita Sundaram</a><br />
    Massachusetts Institute of Technology
  </div>

  <!-- Organizer 5 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://weikaih2004.github.io/">
      <img alt="Weikai Huang" src="pics/organizers/weikaihuang.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://weikaih2004.github.io/">Weikai Huang</a><br />
    University of Washington
  </div>

  <!-- Organizer 6 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://people.csail.mit.edu/weichium/">
      <img alt="Wei-Chiu Ma" src="pics/organizers/weichiuma.png" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://people.csail.mit.edu/weichium/">Wei-Chiu Ma</a><br />
    Cornell University
  </div>



  <!-- Organizer 7 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://web.mit.edu/phillipi/">
      <img alt="Phillip Isola" src="pics/organizers/phillipisola.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://web.mit.edu/phillipi/">Phillip Isola</a><br />
    Massachusetts Institute of Technology
  </div>

  <!-- Organizer 8 -->
  <div style="width:45%; margin: 1%;">
    <a href="https://ranjaykrishna.com/index.html">
      <img alt="Ranjay Krishna" src="pics/organizers/ranjaykrishna.jpg" height="200" width="200" style="border-radius: 50%; object-fit: cover;" />
    </a><br />
    <a href="https://ranjaykrishna.com/index.html">Ranjay Krishna</a><br />
    University of Washington
  </div>
  

</div>


      <footer class="site-footer">
       
      </footer>
    </section>

    
  </body>
</html>
